import asyncio
from typing import List, Dict, Any, TypedDict, Optional
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_anthropic import ChatAnthropic
from tavily import TavilyClient
import concurrent.futures
import json

# Configuration constants
MAX_SEARCH_QUERIES = 5
MAX_SEARCH_RESULTS = 3
MAX_REFLECTION_STEPS = 2

class CompanyInfo(BaseModel):
    """Structured company information to be gathered"""
    company_name: str = Field(description="Official name of the company")
    founding_year: Optional[int] = Field(None, description="Year the company was founded")
    founder_names: List[str] = Field(default_factory=list, description="Names of the founding team members")
    product_description: Optional[str] = Field(None, description="Brief description of the company's main product or service")
    funding_summary: Optional[str] = Field(None, description="Summary of the company's funding history")
    notable_customers: Optional[str] = Field(None, description="Known customers that use company's product/service")

class ResearchState(TypedDict):
    """State for the company research workflow"""
    messages: List[BaseMessage]
    company_name: str
    notes: Optional[str]
    company_info: Optional[CompanyInfo]
    search_queries_used: int
    search_results: List[Dict[str, Any]]
    reflection_count: int
    is_complete: bool

def initialize_research(state: ResearchState) -> Dict[str, Any]:
    """Initialize the research process"""
    messages = state.get("messages", [])
    
    # Extract company name from the latest human message
    if messages:
        latest_message = messages[-1]
        if isinstance(latest_message, HumanMessage):
            content = latest_message.content
            # Simple extraction - in real implementation, you'd parse this more robustly
            company_name = state.get("company_name", "")
            notes = state.get("notes", "")
    
    initial_company_info = CompanyInfo(company_name=state["company_name"])
    
    system_msg = SystemMessage(content=f"""You are a company researcher. Your task is to gather comprehensive information about {state['company_name']}.
    
    User notes: {state.get('notes', 'None provided')}
    
    You need to collect the following information:
    - Company name (official name)
    - Founding year
    - Founder names
    - Product/service description
    - Funding history summary
    - Notable customers
    
    You have access to web search and will work systematically to gather this information.""")
    
    return {
        "messages": add_messages(state["messages"], [system_msg]),
        "company_info": initial_company_info,
        "search_queries_used": 0,
        "search_results": [],
        "reflection_count": 0,
        "is_complete": False
    }

def generate_search_queries(state: ResearchState) -> Dict[str, Any]:
    """Generate search queries to gather company information"""
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    
    current_info = state["company_info"]
    company_name = state["company_name"]
    notes = state.get("notes", "")
    queries_used = state["search_queries_used"]
    remaining_queries = MAX_SEARCH_QUERIES - queries_used
    
    # Determine what information we still need
    missing_info = []
    if not current_info.founding_year:
        missing_info.append("founding year")
    if not current_info.founder_names:
        missing_info.append("founders")
    if not current_info.product_description:
        missing_info.append("products/services")
    if not current_info.funding_summary:
        missing_info.append("funding history")
    if not current_info.notable_customers:
        missing_info.append("customers")
    
    prompt = f"""Generate {min(remaining_queries, 3)} specific search queries to find information about {company_name}.
    
    Missing information: {', '.join(missing_info) if missing_info else 'General company information'}
    User notes: {notes}
    
    Return only the search queries, one per line. Focus on the most important missing information first.
    Make queries specific and likely to return relevant results."""
    
    message = HumanMessage(content=prompt)
    response = model.invoke([message])
    
    # Parse queries from response
    queries = [q.strip() for q in response.content.split('\n') if q.strip()]
    queries = queries[:remaining_queries]  # Respect remaining query limit
    
    search_msg = AIMessage(content=f"Generated {len(queries)} search queries: {', '.join(queries)}")
    
    return {
        "messages": add_messages(state["messages"], [search_msg]),
        "generated_queries": queries
    }

def execute_searches(state: ResearchState) -> Dict[str, Any]:
    """Execute web searches in parallel using Tavily"""
    queries = state.get("generated_queries", [])
    
    if not queries:
        return {"search_results": state["search_results"]}
    
    try:
        tavily_client = TavilyClient()
        
        # Execute searches in parallel
        def search_single_query(query):
            try:
                result = tavily_client.search(
                    query=query,
                    search_depth="advanced",
                    max_results=MAX_SEARCH_RESULTS
                )
                return {"query": query, "results": result.get("results", [])}
            except Exception as e:
                return {"query": query, "results": [], "error": str(e)}
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            search_results = list(executor.map(search_single_query, queries))
        
        # Flatten results
        all_results = []
        for search_result in search_results:
            all_results.extend(search_result["results"])
        
        search_msg = AIMessage(content=f"Executed {len(queries)} searches and found {len(all_results)} results")
        
        return {
            "messages": add_messages(state["messages"], [search_msg]),
            "search_results": state["search_results"] + all_results,
            "search_queries_used": state["search_queries_used"] + len(queries)
        }
        
    except Exception as e:
        error_msg = AIMessage(content=f"Error executing searches: {str(e)}")
        return {
            "messages": add_messages(state["messages"], [error_msg]),
            "search_results": state["search_results"]
        }

def extract_information(state: ResearchState) -> Dict[str, Any]:
    """Extract and update company information from search results"""
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    
    search_results = state["search_results"]
    current_info = state["company_info"]
    company_name = state["company_name"]
    
    if not search_results:
        return {"company_info": current_info}
    
    # Prepare search results context
    results_context = ""
    for i, result in enumerate(search_results[-10:]):  # Use last 10 results
        results_context += f"Result {i+1}:\nTitle: {result.get('title', '')}\nContent: {result.get('content', '')}\nURL: {result.get('url', '')}\n\n"
    
    prompt = f"""Based on the search results below, extract and update information about {company_name}.

Current information:
{current_info.model_dump_json(indent=2)}

Search Results:
{results_context}

Please extract and update the company information. Return the updated information in the same JSON structure.
Only update fields where you have confident, specific information from the search results.
For founder_names, provide a list of individual names.
Keep existing information if no better information is found."""
    
    structured_llm = model.with_structured_output(CompanyInfo)
    message = HumanMessage(content=prompt)
    
    try:
        updated_info = structured_llm.invoke([message])
        
        extract_msg = AIMessage(content=f"Extracted information from {len(search_results)} search results")
        
        return {
            "messages": add_messages(state["messages"], [extract_msg]),
            "company_info": updated_info
        }
        
    except Exception as e:
        error_msg = AIMessage(content=f"Error extracting information: {str(e)}")
        return {
            "messages": add_messages(state["messages"], [error_msg]),
            "company_info": current_info
        }

def reflect_on_completeness(state: ResearchState) -> Dict[str, Any]:
    """Reflect on whether we have sufficient information"""
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    
    company_info = state["company_info"]
    company_name = state["company_name"]
    reflection_count = state["reflection_count"]
    
    # Check completeness
    completeness_score = 0
    total_fields = 6
    
    if company_info.company_name:
        completeness_score += 1
    if company_info.founding_year:
        completeness_score += 1
    if company_info.founder_names:
        completeness_score += 1
    if company_info.product_description:
        completeness_score += 1
    if company_info.funding_summary:
        completeness_score += 1
    if company_info.notable_customers:
        completeness_score += 1
    
    completeness_percentage = (completeness_score / total_fields) * 100
    
    prompt = f"""Evaluate the completeness of our research on {company_name}.

Current information:
{company_info.model_dump_json(indent=2)}

Completeness: {completeness_score}/{total_fields} fields ({completeness_percentage:.1f}%)
Reflection count: {reflection_count}/{MAX_REFLECTION_STEPS}

Is this information sufficient for a comprehensive company profile? 
Consider:
1. Do we have the core business information (product, founding)?
2. Are there major gaps that would make this profile incomplete?
3. Would additional searches likely yield significantly better information?

Respond with 'COMPLETE' if sufficient, or 'CONTINUE' if we need more information."""
    
    message = HumanMessage(content=prompt)
    response = model.invoke([message])
    
    is_complete = (
        "COMPLETE" in response.content.upper() or 
        completeness_percentage >= 70 or 
        reflection_count >= MAX_REFLECTION_STEPS or
        state["search_queries_used"] >= MAX_SEARCH_QUERIES
    )
    
    reflection_msg = AIMessage(content=f"Reflection {reflection_count + 1}: {response.content}")
    
    return {
        "messages": add_messages(state["messages"], [reflection_msg]),
        "reflection_count": reflection_count + 1,
        "is_complete": is_complete
    }

def finalize_research(state: ResearchState) -> Dict[str, Any]:
    """Finalize the research and prepare final output"""
    company_info = state["company_info"]
    
    final_msg = AIMessage(content=f"""Research completed for {company_info.company_name}.

Final Company Profile:
{json.dumps(company_info.model_dump(), indent=2)}

Search queries used: {state['search_queries_used']}/{MAX_SEARCH_QUERIES}
Reflection steps: {state['reflection_count']}/{MAX_REFLECTION_STEPS}
Total search results processed: {len(state['search_results'])}""")
    
    return {
        "messages": add_messages(state["messages"], [final_msg]),
        "is_complete": True
    }

def should_continue_research(state: ResearchState) -> str:
    """Routing function to determine next step"""
    if state["is_complete"]:
        return "finalize"
    elif state["search_queries_used"] >= MAX_SEARCH_QUERIES:
        return "finalize"
    elif state["reflection_count"] >= MAX_REFLECTION_STEPS:
        return "finalize"
    else:
        return "generate_queries"

# Build the graph
def create_company_researcher():
    workflow = StateGraph(ResearchState)
    
    # Add nodes
    workflow.add_node("initialize", initialize_research)
    workflow.add_node("generate_queries", generate_search_queries)
    workflow.add_node("execute_searches", execute_searches)
    workflow.add_node("extract_info", extract_information)
    workflow.add_node("reflect", reflect_on_completeness)
    workflow.add_node("finalize", finalize_research)
    
    # Add edges
    workflow.add_edge(START, "initialize")
    workflow.add_edge("initialize", "generate_queries")
    workflow.add_edge("generate_queries", "execute_searches")
    workflow.add_edge("execute_searches", "extract_info")
    workflow.add_edge("extract_info", "reflect")
    
    # Conditional edge based on reflection
    workflow.add_conditional_edges(
        "reflect",
        should_continue_research,
        {
            "generate_queries": "generate_queries",
            "finalize": "finalize"
        }
    )
    
    workflow.add_edge("finalize", END)
    
    return workflow.compile()

# Export the compiled graph as 'app'
app = create_company_researcher()